# wordcount

现在有100个文本文件，预计一共涉及单词数量1W左右。
需要由5个线程并发计算，全部计算后做结果合并，选出出现频率最高的100个单词及对应次数。

* 五个线程对100个文件进行并发计算，由单词数量1w左右，可以得到容器的大小也在1w左右。
选取世界上最长的单词加10-15个字母，大约60个字符的字符串作为key，测试发现单个Map的大小在900k左右，考虑到实际场景，Map的大小不会这么大，
故在内存允许的情况下，可以直接进行内存计算。如果在生产环境，此计算逻辑建议对业务服务器隔离，并进行分布式并发控制，避免导致频繁gc

* 因为单词分布不一定均匀，因此不能先分别计算5个线程的top100，再进行归并

* 1w+数据选择top100采取了堆排序，时间复杂度O(nlogk), n在此题中是10000, k是100

* 代码实现了两种reduce的模式，一种严格要求5个异步任务全部执行完后才开始进行归并，另一种则不这么严格，会拿到future之后就开始归并